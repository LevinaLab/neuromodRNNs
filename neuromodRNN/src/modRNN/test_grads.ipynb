{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tasks\n",
    "import models\n",
    "import jax\n",
    "import learning_rules\n",
    "import learning_utils\n",
    "from jax import random, numpy as jnp\n",
    "from optax import losses\n",
    "from typing import (\n",
    "  Any,\n",
    "  Callable,\n",
    "  Dict,\n",
    "  List,\n",
    "  Optional,\n",
    "  Sequence,\n",
    "  Tuple,\n",
    "  Iterable  \n",
    " )\n",
    "from flax.typing import (PRNGKey)\n",
    "import optax\n",
    "from flax.training import train_state, orbax_utils\n",
    "Array = jnp.ndarray\n",
    "TrainState = train_state.TrainState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_task = 20\n",
    "n_ALIF=50\n",
    "n_LIF=50\n",
    "n_rec= n_ALIF + n_LIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = models.LSSN(n_ALIF=n_ALIF, n_LIF=n_LIF, n_out=2,thr=0.03,beta=0.018, tau_m=20,tau_out=20,k=0.00, connectivity_rec_layer=\"local\", learning_rule=\"e_prop_autodiff\", sparse_input=True,sparse_readout=True, refractory_period=5,  gain=[2.0,2.0,2.0,2.0,2.0], gridshape=(10,10)) #,sigma=0.003)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_batches = list(tasks.cue_accumulation_task(n_batches=32, batch_size=32, seed=seed_task))\n",
    "# task_batches = list(tasks.pattern_generation(n_batches=1, batch_size=1, seed=seed_task, frequencies=[0.5, 1., 2., 3., 4.],\n",
    "#                                      n_population=100, f_input=10, trial_dur=2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = task_batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization_loss(logits, labels, z, c_reg, f_target, trial_length):    \n",
    "  \"\"\" Loss to be minimized by network, including task loss and any other, e.g. here also firing regularization\n",
    "      Notes:\n",
    "        1. logits is assumed to be non normalized logits\n",
    "        2. labels are assumed to be one-hot encoded\n",
    "  \"\"\"\n",
    "  # notice that optimization_loss is only called inside of learning_rules.compute_grads, and labels are already passed there as one-hot code and y is already softmax transformed\n",
    "  task_loss = jnp.mean(jnp.mean(losses.softmax_cross_entropy(logits=logits, labels=labels), axis=0)) # mean over batches and sum over time\n",
    "  \n",
    "  av_f_rate = learning_utils.compute_firing_rate(z=z, trial_length=trial_length)\n",
    "  f_target = f_target / 1000 # f_target is given in Hz, bu av_f_rate is spikes/ms --> Bellec 2020 used the f_reg also in spikes/ms\n",
    "  regularization_loss = 0.5 * c_reg * jnp.sum(jnp.mean(jnp.square(av_f_rate - f_target), 0)) # average over batches\n",
    "  return task_loss + regularization_loss\n",
    "\n",
    "# def optimization_loss(logits, labels, z, c_reg, f_target, trial_length):\n",
    "    \n",
    "#   if labels.ndim==2: # calling labels what normally people call targets in regression tasks\n",
    "#       labels = jnp.expand_dims(labels, axis=-1) # this is necessary because target labels might have only (n_batch, n_t) and predictions (n_batch, n_t, n_out=1)\n",
    "\n",
    "#   task_loss = 0.5 * jnp.sum(jnp.mean(losses.squared_error(targets=labels, predictions=logits),axis=0))# sum over batches and time --> usually, take average, but biologically is unplausible that updates are averaged across batches, so sum\n",
    "#   #task_loss = 0.5 * jnp.mean(losses.squared_error(targets=labels, predictions=logits))\n",
    "#   av_f_rate = learning_utils.compute_firing_rate(z=z, trial_length=trial_length)\n",
    "#   f_target = f_target / 1000 # f_target is given in Hz, bu av_f_rate is spikes/ms --> Bellec 2020 used the f_reg also in spikes/ms\n",
    "#   regularization_loss = 0.5 * c_reg * jnp.sum(jnp.mean(jnp.square(av_f_rate - f_target),0)) # average over batches\n",
    "#   return task_loss + regularization_loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_params(rng, model, input_shape):\n",
    "  \"\"\"Returns randomly initialized parameters, eligibility parameters and connectivity mask.\"\"\"\n",
    "  dummy_x = jnp.ones(input_shape)\n",
    "  variables = model.init(rng, dummy_x)\n",
    "  return variables['params'], variables['eligibility params'], variables['spatial params']\n",
    "    \n",
    "\n",
    "def get_init_eligibility_carries(rng, model, input_shape):\n",
    "  \"\"\"Returns randomly initialized carries. In the default mode, they are all initialized as zeros arrays\"\"\"\n",
    "  return model.initialize_eligibility_carry(rng, input_shape)\n",
    "\n",
    "def get_init_error_grid(rng, model, input_shape):\n",
    "   \"\"\"Return initial error grid initialized as zeros\"\"\"\n",
    "   return model.initialize_grid(rng=rng, input_shape=input_shape)\n",
    "\n",
    "# Create a custom TrainState to include both params and other variable collections\n",
    "class TrainStateEProp(TrainState):\n",
    "  \"\"\" Personalized TrainState for e-prop with local connectivity \"\"\"\n",
    "  eligibility_params: Dict[str, Array]\n",
    "  spatial_params: Dict[str, Array]\n",
    "  init_eligibility_carries: Dict[str, Array]\n",
    "  init_error_grid: Array\n",
    "  \n",
    "def create_train_state(rng:PRNGKey, learning_rate:float, model, input_shape:Tuple[int,...])->train_state.TrainState:\n",
    "  \"\"\"Create initial training state.\"\"\"\n",
    "  key1, key2, key3 = random.split(rng, 3)\n",
    "  params, eligibility_params, spatial_params = get_initial_params(key1, model, input_shape)\n",
    "  init_eligibility_carries = get_init_eligibility_carries(key2, model, input_shape)\n",
    "  init_error_grid = get_init_error_grid(key3, model, input_shape)\n",
    "\n",
    "  tx = optax.adam(learning_rate=learning_rate)\n",
    "\n",
    "  state = TrainStateEProp.create(apply_fn=model.apply, params=params, tx=tx, \n",
    "                                  eligibility_params=eligibility_params,\n",
    "                                  spatial_params = spatial_params,\n",
    "                                  init_eligibility_carries=init_eligibility_carries,                                  \n",
    "                                  init_error_grid=init_error_grid\n",
    "                                  )\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_1 = create_train_state(random.key(0), learning_rate=0.01, model=model_1, input_shape=(32,40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.where(state_1.params['ALIFCell_0'][\"recurrent_weights\"]!=0,1,0).sum()/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LS_avail = 50\n",
    "c_reg =0.1\n",
    "f_target = 10\n",
    "optimization_loss_fn = optimization_loss\n",
    "task = \"classification\"\n",
    "learning_rule = \"e_prop_hardcoded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_1, grads_1 = learning_rules.compute_grads(batch=batch, state=state_1,optimization_loss_fn=optimization_loss_fn,\n",
    "                                                  LS_avail=LS_avail, f_target=f_target, c_reg=c_reg, task=task, learning_rule=\"e_prop_autodiff\",\n",
    "                                                  shuffle=False, key=random.key(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_hard_1,hard_grads_1 = learning_rules.compute_grads(batch=batch, state=state_1,optimization_loss_fn=optimization_loss_fn,\n",
    "                                                  LS_avail=LS_avail, f_target=f_target, c_reg=c_reg,\n",
    "                                                   learning_rule=learning_rule, task=task,\n",
    "                                                  shuffle=True, key=random.key(0))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_grads_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_out_1 = grads_1[\"ReadOut_0\"][\"readout_weights\"]\n",
    "read_out_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_out_hard_1 = hard_grads_1[\"ReadOut_0\"][\"readout_weights\"]\n",
    "read_out_hard_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recurrent_1 = grads_1['ALIFCell_0'][\"input_weights\"]\n",
    "mask = jnp.where(recurrent_1!=0.)\n",
    "recurrent_1[mask]\n",
    "recurrent_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recurrent_hard_1 = hard_grads_1['ALIFCell_0'][\"input_weights\"]\n",
    "hardcoded_mask = jnp.where(recurrent_hard_1!=0.)\n",
    "recurrent_hard_1[hardcoded_mask]\n",
    "jnp.max(jnp.abs(recurrent_hard_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct = jnp.absolute(recurrent_hard_1[mask]-recurrent_1[mask]) < 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.max(jnp.abs(recurrent_hard_1-recurrent_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.where(state_1.params[\"ALIFCell_0\"][\"input_weights\"]!=0., 1,0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "n_b=10\n",
    "n_in=100\n",
    "n_rec=400\n",
    "# Assuming gradients is a JAX array of shape (n_b, n_in, n_rec)\n",
    "gradients = jax.random.normal(jax.random.PRNGKey(0), (n_b, n_in, n_rec))  # Example data\n",
    "\n",
    "# Step 1: Flatten the matrix along the last two dimensions\n",
    "flattened_grads = gradients.reshape((n_b, -1))  # New shape: (n_b, n_in * n_rec)\n",
    "\n",
    "# Step 2: Compute cosine similarity\n",
    "# Normalize the flattened gradients\n",
    "norms = jnp.linalg.norm(flattened_grads, axis=1, keepdims=True)\n",
    "normalized_grads = flattened_grads / norms\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "similarity = jnp.dot(normalized_grads, normalized_grads.T)\n",
    "\n",
    "# similarity now has shape (n_b, n_b), representing cosine similarity between each pair of batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Assuming gradients_a and gradients_b are both JAX arrays of shape (n_b, n_in, n_rec)\n",
    "grad_a = jax.random.normal(jax.random.PRNGKey(0), (n_in, n_rec))  # Example data# Example data\n",
    "grad_b = jax.random.normal(jax.random.PRNGKey(1), (n_in, n_rec))  # Example data# Example data\n",
    "\n",
    "# Step 1: Flatten the gradients along the last two dimensions\n",
    "flattened_a = grad_a.reshape(-1)  # Shape: (n_in * n_rec,)\n",
    "flattened_b = grad_b.reshape(-1)  # Shape: (n_in * n_rec,)\n",
    "\n",
    "# Step 2: Compute cosine similarity\n",
    "# Normalize the gradients\n",
    "norm_a = jnp.linalg.norm(flattened_a)  # Scalar value for the norm of grad_a\n",
    "norm_b = jnp.linalg.norm(flattened_b)  # Scalar value for the norm of grad_b\n",
    "\n",
    "# Avoid dividing by zero in case of zero norms\n",
    "norm_a = jnp.where(norm_a == 0, 1e-10, norm_a)\n",
    "norm_b = jnp.where(norm_b == 0, 1e-10, norm_b)\n",
    "\n",
    "# Normalize the vectors\n",
    "normalized_a = flattened_a / norm_a\n",
    "normalized_b = flattened_b / norm_b\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_similarity = jnp.sum(normalized_a * normalized_b)  # Scalar value\n",
    "\n",
    "# Clip cosine similarity to be within the range [-1, 1] to avoid numerical issues with arccos\n",
    "cosine_similarity = jnp.clip(cosine_similarity, -1.0, 1.0)\n",
    "\n",
    "# Step 3: Compute the angle in radians\n",
    "angle_in_radians = jnp.arccos(cosine_similarity)  # Scalar value\n",
    "\n",
    "# If you want the angle in degrees (optional)\n",
    "angle_in_degrees = jnp.degrees(angle_in_radians)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optax.losses import cosine_similarity\n",
    "cosine_similarity(flattened_a, flattened_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plots\n",
    "\n",
    "layer_names = [\"Input layer\", \"Recurrent layer\", \"Readout layer\"]\n",
    "plots.plot_LSNN_weights(state_1,layer_names=layer_names,\n",
    "                    save_path=r\"C:\\Users\\j1559\\Documents\\Tuebingen\\SS_24\\MasterThesis\\neuromodRNNs\\neuromodRNN\\src\\modRNN\\weights.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modRNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
