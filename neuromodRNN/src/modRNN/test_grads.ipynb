{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tasks\n",
    "import models\n",
    "\n",
    "import learning_rules\n",
    "import learning_utils\n",
    "from jax import random, numpy as jnp\n",
    "from optax import losses\n",
    "from typing import (\n",
    "  Any,\n",
    "  Callable,\n",
    "  Dict,\n",
    "  List,\n",
    "  Optional,\n",
    "  Sequence,\n",
    "  Tuple,\n",
    "  Iterable  \n",
    " )\n",
    "from flax.typing import (PRNGKey)\n",
    "import optax\n",
    "from flax.training import train_state, orbax_utils\n",
    "Array = jnp.ndarray\n",
    "TrainState = train_state.TrainState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_task = 2\n",
    "n_ALIF=0\n",
    "n_LIF=100\n",
    "n_rec= n_ALIF + n_LIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = models.LSSN(n_ALIF=n_ALIF, n_LIF=n_LIF, n_out=2,thr=0.01,tau_m=30,tau_out=30, local_connectivity=True, learning_rule=\"e_prop_autodiff\", sparse_connectivity=True, refractory_period=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_batches = list(tasks.cue_accumulation_task(n_batches=8, batch_size=8, seed=seed_task))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = task_batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization_loss(logits, labels, z, c_reg, f_target, trial_length):\n",
    "    \n",
    "# notice that optimization_loss is only called inside of learning_rules.compute_grads, and labels are already passed there as one-hot code and y is already softmax transformed\n",
    "  task_loss = jnp.sum(losses.softmax_cross_entropy(logits=logits, labels=labels) ) # sum over batches and time --> it accumulates gradients, but in additively way (should not normalize batches)\n",
    "  \n",
    "  av_f_rate = learning_utils.compute_firing_rate(z=z, trial_length=trial_length)\n",
    "  f_target = f_target / 1000 # f_target is given in Hz, bu av_f_rate is spikes/ms --> Bellec 2020 used the f_reg also in spikes/ms\n",
    "  regularization_loss = 0.5 * c_reg * jnp.sum(jnp.mean(jnp.square(av_f_rate - f_target), 0)) # average over batches\n",
    "\n",
    "  return task_loss + regularization_loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_params(rng, model, input_shape):\n",
    "  \"\"\"Returns randomly initialized parameters, eligibility parameters and connectivity mask.\"\"\"\n",
    "  dummy_x = jnp.ones(input_shape)\n",
    "  variables = model.init(rng, dummy_x)\n",
    "  return variables['params'], variables['eligibility params'], variables['spatial params']\n",
    "    \n",
    "\n",
    "def get_init_eligibility_carries(rng, model, input_shape):\n",
    "  \"\"\"Returns randomly initialized carries. In the default mode, they are all initialized as zeros arrays\"\"\"\n",
    "  return model.initialize_eligibility_carry(rng, input_shape)\n",
    "\n",
    "def get_init_error_grid(rng, model, input_shape):\n",
    "   \"\"\"Return initial error grid initialized as zeros\"\"\"\n",
    "   return model.initialize_grid(rng=rng, input_shape=input_shape)\n",
    "\n",
    "# Create a custom TrainState to include both params and other variable collections\n",
    "class TrainStateEProp(TrainState):\n",
    "  \"\"\" Personalized TrainState for e-prop with local connectivity \"\"\"\n",
    "  eligibility_params: Dict[str, Array]\n",
    "  spatial_params: Dict[str, Array]\n",
    "  init_eligibility_carries: Dict[str, Array]\n",
    "  init_error_grid: Array\n",
    "  \n",
    "def create_train_state(rng:PRNGKey, learning_rate:float, model, input_shape:Tuple[int,...])->train_state.TrainState:\n",
    "  \"\"\"Create initial training state.\"\"\"\n",
    "  key1, key2, key3 = random.split(rng, 3)\n",
    "  params, eligibility_params, spatial_params = get_initial_params(key1, model, input_shape)\n",
    "  init_eligibility_carries = get_init_eligibility_carries(key2, model, input_shape)\n",
    "  init_error_grid = get_init_error_grid(key3, model, input_shape)\n",
    "\n",
    "  tx = optax.adam(learning_rate=learning_rate)\n",
    "\n",
    "  state = TrainStateEProp.create(apply_fn=model.apply, params=params, tx=tx, \n",
    "                                  eligibility_params=eligibility_params,\n",
    "                                  spatial_params = spatial_params,\n",
    "                                  init_eligibility_carries=init_eligibility_carries,                                  \n",
    "                                  init_error_grid=init_error_grid\n",
    "                                  )\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_1 = create_train_state(random.key(0), learning_rate=0.01, model=model_1, input_shape=(8,40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_1.spatial_params[\"ALIFCell_0\"][\"cells_loc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_1.eligibility_params['ReadOut_0']['feedback_weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_1.params['ReadOut_0']['readout_weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LS_avail = 1\n",
    "c_reg =0.1\n",
    "f_target = 10\n",
    "optimization_loss_fn = optimization_loss\n",
    "task = \"classification\"\n",
    "local_connectivity = True\n",
    "learning_rule = \"e_prop_hardcoded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_1, grads_1 = learning_rules.compute_grads(batch=batch, state=state_1,optimization_loss_fn=optimization_loss_fn,\n",
    "                                                  LS_avail=LS_avail, local_connectivity=local_connectivity, \n",
    "                                                  f_target=f_target, c_reg=c_reg, task=task, learning_rule=\"e_prop_autodiff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_hard_1,hard_grads_1 = learning_rules.compute_grads(batch=batch, state=state_1,optimization_loss_fn=optimization_loss_fn,\n",
    "                                                  LS_avail=LS_avail, local_connectivity=local_connectivity, \n",
    "                                                  f_target=f_target, c_reg=c_reg, learning_rule=learning_rule, task=task)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_grads_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_out_1 = grads_1[\"ReadOut_0\"][\"readout_weights\"]\n",
    "read_out_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_out_hard_1 = hard_grads_1[\"ReadOut_0\"][\"readout_weights\"]\n",
    "read_out_hard_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recurrent_1 = grads_1['ALIFCell_0'][\"input_weights\"]\n",
    "mask = jnp.where(recurrent_1!=0.)\n",
    "recurrent_1[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recurrent_hard_1 = hard_grads_1['ALIFCell_0'][\"input_weights\"]\n",
    "mask = jnp.where(recurrent_hard_1!=0.)\n",
    "recurrent_hard_1[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct = jnp.absolute(recurrent_hard_1[mask]-recurrent_1[mask]) < 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.max(recurrent_hard_1[mask]-recurrent_1[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modRNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
